{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33316715-7c0f-4fde-8a0a-f473605f984b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812b59-adad-4ddd-8211-9076dcfd78cc",
   "metadata": {},
   "source": [
    "#### If necessary: Sampling lib - https://little-ball-of-fur.readthedocs.io/en/latest/notes/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e1a44-bab8-45fb-97bf-2f83e207ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from littleballoffur import RandomNodeSampler, ForestFireSampler\n",
    "from littleballoffur import RandomWalkSampler\n",
    "import sys\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "import csv\n",
    "import random\n",
    "import pickle as pkl\n",
    "# import community\n",
    "# from community import community_louvain\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import igraph as ig\n",
    "import pickle\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import pandas as pd\n",
    "from igraph.clustering import compare_communities\n",
    "import sys, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194088b-4d2c-4b5a-b552-f8b899224ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_path = 'whatsapp.edgelist'  #Insert here dataset edgelist file path (in this code we have whatsapp example)\n",
    "G = nx.read_weighted_edgelist(edgelist_path, delimiter=\" \") #Read \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724b5df-7bf6-487d-a826-f7ada9877d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = G\n",
    "number_of_nodes = graph.number_of_nodes()\n",
    "print(number_of_nodes)\n",
    "print(graph.number_of_edges())\n",
    "# validate dataset number of edges and nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33ef85-f1fa-4b81-b119-5dfab9155624",
   "metadata": {},
   "source": [
    "# Original Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5969955-91ea-4e66-844a-40c3b8d7aa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Execute 10x community detection in original network\n",
    "\n",
    "##### I store in a csv to have a backup for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70ae4b-3a81-4622-85f2-10ab37110862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_network_executions(G, Path_Communities, header,index):\n",
    "    list_results = []\n",
    "                \n",
    "    n_nodes = len(G.nodes())\n",
    "    n_edges = len(G.edges())\n",
    "    \n",
    "    degree = dict(G.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G)\n",
    "    components = list(nx.connected_components(G))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G),4)\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    # Calculate Louvain community detection\n",
    "    partition = community.best_partition(G, resolution=1)\n",
    "    modularity = community.modularity(partition, G)\n",
    "    \n",
    "    partition = {node: comm_id for node, comm_id in partition.items() if list(partition.values()).count(comm_id) > 10}\n",
    "    \n",
    "    pkl.dump(partition, open(Path_Communities, \"wb\"), protocol=4)\n",
    "    \n",
    "    n_comm = len(set(partition.values()))\n",
    "    community_sizes = Counter(partition.values())\n",
    "    biggest_community_size = max(community_sizes.values())\n",
    "    \n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G)\n",
    "    \n",
    "    parameter = '-'\n",
    "    sample = \"-\"\n",
    "    batch = \"-\"\n",
    "    intersection = \"-\"\n",
    "    nmi = \"-\"\n",
    "    \n",
    "    network = 'whatsapp' # network name\n",
    "    type_network = \"original_\"+str(i)\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm,biggest_community_size, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "\n",
    "    df.to_csv(\"network/original/network_whatsapp_original_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40e1e7-47dd-426d-acca-9f4cad69e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = True\n",
    "for i in range(1, 11):\n",
    "    characterize_network_executions(graph, 'whatsapp/original/comm_complete_network_'+str(i)+'.pkl', header, i)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee3510-fa9a-4253-8fea-854ea419a87e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Compare communities of the executions and calculate NMI mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273a66e-34c6-4896-a6ee-d41f778d83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tamanho_comunidade = 10\n",
    "PATH_Communities = 'whatsapp/original/'\n",
    "list_results = []\n",
    "list_models = ['comm_complete_whatsapp_1', 'comm_complete_whatsapp_2', \n",
    "               'comm_complete_whatsapp_3', 'comm_complete_whatsapp_4', \n",
    "               'comm_complete_whatsapp_5', 'comm_complete_whatsapp_6', \n",
    "               'comm_complete_whatsapp_7', 'comm_complete_whatsapp_8',\n",
    "               'comm_complete_whatsapp_9', 'comm_complete_whatsapp_10']\n",
    "\n",
    "for i in range(1,len(list_models),1):\n",
    "    model_1 = list_models[0]\n",
    "    model_2 = list_models[i]\n",
    "  \n",
    "    # Load the pickled communities\n",
    "    with open(PATH_Communities + model_1 + '.pkl', 'rb') as file:\n",
    "        dicionario1 = pickle.load(file)\n",
    "    with open(PATH_Communities + model_2 + '.pkl', 'rb') as file:\n",
    "        dicionario2 = pickle.load(file)\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "\n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "\n",
    "    # Obter as chaves em comum entre os dicionários filtrados\n",
    "    chaves_comuns = set(comunidades_filtradas1.keys()) & set(comunidades_filtradas2.keys())\n",
    "    intersection = len(chaves_comuns)\n",
    "    # print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "\n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "\n",
    "    nmi = 0\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    if(intersection> 1):\n",
    "        nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "\n",
    "    list_results.append((model_1, model_2, nmi))\n",
    "\n",
    "df = pd.DataFrame(list_results, columns = ['Model 1', 'Model 2', 'NMI'])\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "csv_file = 'whatsapp/original/nmi_original_whatsapp_results.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "df\n",
    "\n",
    "mean_nmi = df[\"NMI\"].mean()\n",
    "std_nmi = df[\"NMI\"].std()\n",
    "\n",
    "# Create a new DataFrame with the mean and std values\n",
    "result_df = pd.DataFrame({\"NMI\": ['{:.4f}±{:.4f}'.format(mean_nmi,std_nmi)]})\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "csv_file = 'whatsapp/original/nmi_mean_whatsapp_results.csv'\n",
    "result_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13133a2c-a252-413a-8287-1eceac0a453d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 - Execute Random Node Method to generate original network samples (we have generated 10samples for each param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db850c12-944f-4870-bef0-6afb475d0710",
   "metadata": {},
   "source": [
    "#### Created relabel method to handle limitation of networkX\n",
    "#### Random Node Reference: https://little-ball-of-fur.readthedocs.io/en/latest/_modules/littleballoffur/node_sampling/randomnodesampler.html#RandomNodeSampler\n",
    "#### I also stored samples to be able to detect its communities next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe499ea-6dbd-4a91-ac0c-2c28a287b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_nodes(graph):\n",
    "    nodes = sorted(graph.nodes)\n",
    "    relabel_mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "    relabeled_graph = nx.relabel_nodes(graph, relabel_mapping)\n",
    "    \n",
    "    # Update node weights in the relabeled graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if u in relabel_mapping and v in relabel_mapping:\n",
    "            u_mapped = relabel_mapping[u]\n",
    "            v_mapped = relabel_mapping[v]\n",
    "            if u_mapped in relabeled_graph and v_mapped in relabeled_graph[u_mapped]:\n",
    "                relabeled_graph[u_mapped][v_mapped]['weight'] = data['weight']\n",
    "    \n",
    "    return relabeled_graph, relabel_mapping\n",
    "\n",
    "def revert_relabel_nodes(graph, relabel_mapping):\n",
    "    original_mapping = {v: k for k, v in relabel_mapping.items()}\n",
    "    reverted_graph = nx.relabel_nodes(graph, original_mapping)\n",
    "    return reverted_graph\n",
    "\n",
    "def run_random_node_sampling_model(graph_nx, dataset):\n",
    "    j = 1\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Step 1: Apply relabel_nodes(graph) to weighted graph_nx\n",
    "    relabeled_graph_nx, relabel_mapping = relabel_nodes(graph_nx)\n",
    "    \n",
    "    while j <= 10:\n",
    "        print(j)\n",
    "        values_seed = []\n",
    "        seed = random.randint(0, 4294967294)\n",
    "        \n",
    "        for i in range(1, 11, 1):\n",
    "            random_node_model = RandomNodeSampler(number_of_nodes=int((i/10)*relabeled_graph_nx.number_of_nodes()), seed=seed)\n",
    "            \n",
    "            # Step 2: Use the weighted relabeled_graph to calculate sample\n",
    "            sample_random_node = random_node_model.sample(relabeled_graph_nx)\n",
    "            \n",
    "            # Step 3: Revert relabeling to original nodes mapped in relabel_mapping\n",
    "            relabeled_sample_random_node = revert_relabel_nodes(sample_random_node, relabel_mapping)\n",
    "            \n",
    "            result = graph_nx.subgraph(relabeled_sample_random_node.nodes()).edges() == relabeled_sample_random_node.edges()\n",
    "            print(result)\n",
    "                \n",
    "            df_random_node = nx.to_pandas_edgelist(relabeled_sample_random_node, source='source', target='target', nodelist=list(relabeled_sample_random_node.nodes), dtype=None, edge_key=None)\n",
    "\n",
    "            df_random_node.to_csv(\"whatsapp/RandomNode/samples/\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv',index=False)\n",
    "            print(\"saved_\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_'+str(seed)+'_.csv')\n",
    "            \n",
    "            values_seed.append(seed)\n",
    "        \n",
    "        df['batch_seed_'+str(j)] = values_seed\n",
    "        df.to_csv(\"whatsapp/RandomNode/samples/whatsapp_seeds.csv\",index=False)\n",
    "        del seed\n",
    "        j += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1557c9c-43b1-4cea-acd5-443d77f6ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_random_node_sampling_model(G, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adf99a-2825-4d24-ac6e-df42cc96c280",
   "metadata": {},
   "source": [
    "## 3. Detect community of original network generated samples - Random Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22fd6e-8ac7-4554-af6b-01eea029db61",
   "metadata": {},
   "source": [
    "##### In this scenario I have created a separeted method called characterize_random_node_sample_networks to call characterize_sampled_network to detect the  communities of each sample and, after that, I call in it a separated method calculate_nmi to calcutate NMI of the samples.\n",
    "##### I also store the communities info in a separated files for further analysis and I put all the samples info in another csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff17e07-02bb-4ec1-beaa-6b654b3a8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nmi(dicionario1_pkl, dicionario2_pkl, min_tamanho_comunidade=10):\n",
    "    #from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "    # Carregar os dicionários a partir dos arquivos pickle\n",
    "    with open(dicionario1_pkl, 'rb') as arquivo1:\n",
    "        dict1 = pkl.load(arquivo1)\n",
    "    \n",
    "    with open(dicionario2_pkl, 'rb') as arquivo2:\n",
    "        dict2 = pkl.load(arquivo2)\n",
    "        \n",
    "    dicionario1 = {int(key): value for key, value in dict1.items()}\n",
    "    dicionario2 = {int(key): value for key, value in dict2.items()}\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "    \n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "    \n",
    "    chaves_comuns = list(set(comunidades_filtradas1.keys()).intersection(comunidades_filtradas2.keys()))\n",
    "    \n",
    "    intersection = len(chaves_comuns)\n",
    "    print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "    \n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "    \n",
    "    if(intersection <= 1): return 0, 0\n",
    "\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "    print(nmi)\n",
    "    \n",
    "    return nmi, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c40f0-3ac5-46b3-be02-9a794b6e9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sampled_network(graph, Path_Communities, network, type_network, snap, s, parameter, header, sample, batch):\n",
    "    \n",
    "    #Para rede amostrada\n",
    "    list_results = []\n",
    "    print(graph)\n",
    "    df = pd.read_csv(graph)\n",
    "    Graphtype = nx.Graph()\n",
    "    G_sampled = nx.from_pandas_edgelist(df, edge_attr='weight', create_using=Graphtype)\n",
    "    \n",
    "    print(df[:3])\n",
    "    \n",
    "    for component in list(nx.connected_components(G_sampled)):\n",
    "        if len(component)<3:\n",
    "            for node in component:\n",
    "                G_sampled.remove_node(node)\n",
    "                \n",
    "    n_nodes = len(G_sampled.nodes())\n",
    "    n_edges = len(G_sampled.edges())\n",
    "    \n",
    "    degree = dict(G_sampled.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G_sampled, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G_sampled),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G_sampled)\n",
    "    components = list(nx.connected_components(G_sampled))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G_sampled),4)\n",
    "    clustering_coeffs = nx.clustering(G_sampled)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    \n",
    "    complete_partition_sampled = community.best_partition(G_sampled, resolution=1)\n",
    "    modularity = community.modularity(complete_partition_sampled, G_sampled)\n",
    "    \n",
    "    pkl.dump(complete_partition_sampled, open(Path_Communities, \"wb\"), protocol=4) \n",
    "    \n",
    "    n_comm_10 = len(set(complete_partition_sampled.values()))\n",
    "    \n",
    "    community_sizes_10 = Counter(complete_partition_sampled.values())\n",
    "    \n",
    "    biggest_community_size_10 = max(community_sizes_10.values())\n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G_sampled)\n",
    "    \n",
    "    # Calcula o NMI entre os arquivos pickle\n",
    "    nmi, intersection = calculate_nmi('whatsapp/original/comm_complete_whatsapp_1.pkl', Path_Communities)\n",
    "    \n",
    "    parameter = parameter\n",
    "    sample = sample\n",
    "    batch = batch\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"sample_RN\"\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm_10,biggest_community_size_10, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "    del G_sampled\n",
    "    # del G\n",
    "    # del partition\n",
    "    del complete_partition_sampled\n",
    "\n",
    "    df.to_csv(\"whatsapp/RandomNode/communities/networks_whatsapp_samples_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d45701-944d-4705-af5c-31682ed6a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_random_node_sample_networks(graph, dataset):\n",
    "    header = True\n",
    "    j = 1\n",
    "    \n",
    "    while j <= 10:\n",
    "        print(j)\n",
    "        for i in range(1,11,1):\n",
    "            sample = \"whatsapp/RandomNode/samples/\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv'\n",
    "            Path_Communities = \"whatsapp/RandomNode/communities/\"+dataset+\"_RN_batch_\"+str(j)+\"_net_percent_\"+str(i/10)+\".pkl\"\n",
    "            characterize_sampled_network(sample, Path_Communities, dataset, \"sample whatsapp - Random Node\", \"sample_\"+str(i)+\"_batch_\"+str(j), 1, 0, header, str(i/10), str(j))\n",
    "            # print(\"saved_community_\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv')\n",
    "            header = False\n",
    "        header = False\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df885abe-5da2-4132-bbb4-112524a95daa",
   "metadata": {},
   "source": [
    "### 3.1 Calculate samples NMI mean and std per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da014c8b-2829-45d2-899c-6357903ac445",
   "metadata": {},
   "source": [
    "#### In this code I load csv with samples and samples communities info and I group them per parameter and after that I calculate NMI mean and std\n",
    "#### At the end I store them in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214dde9-c312-4552-928d-aa6b5fd596b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_node_original_whatsapp = pd.read_csv('RandomNode/communities/networks_whatsapp_samples_characterization.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ec88d-f7a6-4e42-818d-8e23e6fbec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp = df_random_node_original_whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698369c-fc9c-40d2-8f1b-b5b66f90c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547d90c-d16c-479d-a420-ace97eee4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[\"nmi_mean\"]=0\n",
    "df_RN_NMI_whatsapp[\"std\"]=0\n",
    "df_RN_NMI_whatsapp[\"nmi_std\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e30ae2-33f8-4215-b242-62c38b03eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['nmi_norm'] = df_RN_NMI_whatsapp['nmi'] / 0.4552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907943a-b809-4294-be52-c310fd087858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['nmi_mean'] = df_RN_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895ce86-a16c-4808-8c0c-267f754054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['std'] = df_RN_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152ecd4-3d7e-4bd8-b2f9-1f929838730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[\"nmi_std\"] = df_RN_NMI_whatsapp.apply(lambda x: '{:.4f}±{:.4f}'.format(x[\"nmi_mean\"], x[\"std\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7762079-3daf-4422-bdbb-36e5202485fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = df_RN_NMI_whatsapp[['Batch', 'Net. Sample','nmi_std']].copy()\n",
    "new_df_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308872d7-75a3-4bcd-a149-6a8f9e0a28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2 = new_df_2.pivot(index=[\"Batch\"], columns=\"Net. Sample\")\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b1e76-0cc6-4cb4-b912-23dd450acba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1].to_csv(\"RandomNode/communities/networks_nmi_whatsapp_samples_normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf86898-0a0e-4c22-b19c-47d071f96e93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 - Execute Forest Fire Method to generate original network samples (we have generated 10 samples for each param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c35b77-8c6d-4632-8602-5aa02664f46b",
   "metadata": {},
   "source": [
    "#### Created relabel method to handle limitation of networkX\n",
    "#### Forest Fire Reference: https://little-ball-of-fur.readthedocs.io/en/latest/_modules/littleballoffur/exploration_sampling/forestfiresampler.html#ForestFireSampler\n",
    "#### I also stored samples to be able to detect its communities next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb93b3-7f5f-4e2b-a488-4f62b4a6c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_nodes(graph):\n",
    "    nodes = sorted(graph.nodes)\n",
    "    relabel_mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "    relabeled_graph = nx.relabel_nodes(graph, relabel_mapping)\n",
    "    \n",
    "    # Update node weights in the relabeled graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if u in relabel_mapping and v in relabel_mapping:\n",
    "            u_mapped = relabel_mapping[u]\n",
    "            v_mapped = relabel_mapping[v]\n",
    "            if u_mapped in relabeled_graph and v_mapped in relabeled_graph[u_mapped]:\n",
    "                relabeled_graph[u_mapped][v_mapped]['weight'] = data['weight']\n",
    "    \n",
    "    return relabeled_graph, relabel_mapping\n",
    "\n",
    "def revert_relabel_nodes(graph, relabel_mapping):\n",
    "    original_mapping = {v: k for k, v in relabel_mapping.items()}\n",
    "    reverted_graph = nx.relabel_nodes(graph, original_mapping)\n",
    "    return reverted_graph\n",
    "\n",
    "def run_forest_fire_sampling_model(graph_nx,dataset):\n",
    "    k = 1\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    relabeled_graph_nx, relabel_mapping = relabel_nodes(graph_nx)\n",
    "    \n",
    "    while k <= 10:\n",
    "        print(k)\n",
    "        for i in range(1,11,1):\n",
    "            values_seed = []\n",
    "            for j in range(1,10,1):\n",
    "                seed = random.randint(0, 4294967294)\n",
    "                forest_fire_model = ForestFireSampler(number_of_nodes=int((i/10)*relabeled_graph_nx.number_of_nodes()),p=(j/10),seed=seed)\n",
    "                \n",
    "                sample_forest_fire = forest_fire_model.sample(relabeled_graph_nx)\n",
    "                \n",
    "                relabeled_sample_forest_fire = revert_relabel_nodes(sample_forest_fire, relabel_mapping)\n",
    "            \n",
    "                result = graph_nx.subgraph(relabeled_sample_forest_fire.nodes()).edges() == relabeled_sample_forest_fire.edges()\n",
    "                print(result)\n",
    "                \n",
    "                df_forest_fire_node = nx.to_pandas_edgelist(relabeled_sample_forest_fire, source='source', target='target', nodelist=list(relabeled_sample_forest_fire.nodes), dtype=None, edge_key=None)\n",
    "                df_forest_fire_node.to_csv(\"whatsapp/ForestFire/samples/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv',index=False)\n",
    "                print(\"whatsapp/ForestFire/samples/\"+\"dataset_\"+dataset+\"/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv')\n",
    "                del forest_fire_model\n",
    "                del sample_forest_fire\n",
    "                del df_forest_fire_node\n",
    "                values_seed.append(seed)\n",
    "            df['batch_seed_'+str(k)] = values_seed\n",
    "            df.to_csv(\"whatsapp/ForestFire/samples/\"+dataset+\"_forest_fire_seeds.csv\",index=False)\n",
    "            print(values_seed)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c20b4a-6af4-4684-8357-01adab0c46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_forest_fire_sampling_model(G, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4da086-5f9b-47cd-8985-32ea1303a8d2",
   "metadata": {},
   "source": [
    "## 5. Detect community of original network generated samples - Forest Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21cb93-fc78-4257-8e3a-45bbfd5b3b37",
   "metadata": {},
   "source": [
    "##### In this scenario I have created a separeted method called characterize_forest_fire_sample_networks to call characterize_sampled_network_ff to detect the  communities of each sample and, after that, I call in it a separated method calculate_nmi to calcutate NMI of the samples.\n",
    "##### I also store the communities info in a separated files for further analysis and I put all the samples info in another csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c520d-c845-4f73-8a26-6836e251c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nmi(dicionario1_pkl, dicionario2_pkl, min_tamanho_comunidade=10):\n",
    "    #from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "    # Carregar os dicionários a partir dos arquivos pickle\n",
    "    with open(dicionario1_pkl, 'rb') as arquivo1:\n",
    "        dict1 = pkl.load(arquivo1)\n",
    "    \n",
    "    with open(dicionario2_pkl, 'rb') as arquivo2:\n",
    "        dict2 = pkl.load(arquivo2)\n",
    "        \n",
    "    dicionario1 = {int(key): value for key, value in dict1.items()}\n",
    "    dicionario2 = {int(key): value for key, value in dict2.items()}\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "    \n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "    \n",
    "    chaves_comuns = list(set(comunidades_filtradas1.keys()).intersection(comunidades_filtradas2.keys()))\n",
    "    \n",
    "    intersection = len(chaves_comuns)\n",
    "    print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "    \n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "    \n",
    "    if(intersection < 1): return 0, 0\n",
    "\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "    print(nmi)\n",
    "    \n",
    "    return nmi, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35aeffa-9879-4cc3-a60a-08f9322af555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sampled_network_ff(graph, Path_Communities, network, type_network, snap, s, parameter, header, sample, batch):\n",
    "    \n",
    "    #Para rede amostrada\n",
    "    list_results = []\n",
    "    print(graph)\n",
    "    df = pd.read_csv(graph)\n",
    "    Graphtype = nx.Graph()\n",
    "    G_sampled = nx.from_pandas_edgelist(df, edge_attr='weight', create_using=Graphtype)\n",
    "    \n",
    "    print(df[:3])\n",
    "    \n",
    "    # for component in list(nx.connected_components(G_sampled)):\n",
    "    #     if len(component)<3:\n",
    "    #         for node in component:\n",
    "    #             G_sampled.remove_node(node)\n",
    "                \n",
    "    n_nodes = len(G_sampled.nodes())\n",
    "    n_edges = len(G_sampled.edges())\n",
    "    \n",
    "    degree = dict(G_sampled.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G_sampled, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G_sampled),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G_sampled)\n",
    "    components = list(nx.connected_components(G_sampled))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G_sampled),4)\n",
    "    clustering_coeffs = nx.clustering(G_sampled)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    \n",
    "    complete_partition_sampled = community.best_partition(G_sampled, resolution=1)\n",
    "    modularity = community.modularity(complete_partition_sampled, G_sampled)\n",
    "    \n",
    "    pkl.dump(complete_partition_sampled, open(Path_Communities, \"wb\"), protocol=4) \n",
    "    \n",
    "    n_comm_10 = len(set(complete_partition_sampled.values()))\n",
    "    \n",
    "    community_sizes_10 = Counter(complete_partition_sampled.values())\n",
    "    \n",
    "    biggest_community_size_10 = max(community_sizes_10.values())\n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G_sampled)\n",
    "    \n",
    "    # Calcula o NMI entre os arquivos pickle\n",
    "    nmi, intersection = calculate_nmi('whatsapp/original/comm_complete_whatsapp_1.pkl', Path_Communities)\n",
    "    \n",
    "    parameter = parameter\n",
    "    sample = sample\n",
    "    batch = batch\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"sample_FF\"\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm_10,biggest_community_size_10, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "    del G_sampled\n",
    "    # del G\n",
    "    # del partition\n",
    "    del complete_partition_sampled\n",
    "\n",
    "    df.to_csv(\"whatsapp/ForestFire/communities/networks_whatsapp_samples_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7c708-434e-422a-8c7b-f7b44507e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_forest_fire_sample_networks(graph, dataset):\n",
    "    header = True\n",
    "    k = 1\n",
    "    while k <= 10:\n",
    "        print(k)\n",
    "        for i in range(1,11,1):\n",
    "            for j in range(1,10,1):\n",
    "                sample = \"whatsapp/ForestFire/samples/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv'\n",
    "                Path_Communities = \"whatsapp/ForestFire/communities/\"+dataset+\"_FF_batch_\"+str(k)+\"_net_percent_\"+str(i/10)+\"_probability_\"+str(j/10)+\".pkl\"\n",
    "                characterize_sampled_network_ff(sample, Path_Communities, dataset, \"sample Whatsapp - Forest Fire\", \"batch_\"+str(k)+'_sample_'+str(i/10)+'_probaability_'+str(j/10), 1, str(j/10), header, str(i/10), str(k))\n",
    "                header = False\n",
    "            # print(\"saved_community_\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch_'+str(k)+'_'+str(i)+'_.csv')\n",
    "            header = False\n",
    "        header = False\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2be572-ef1e-481c-992a-73417849c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characterize_forest_fire_sample_networks(graph, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce7ed3f-6476-4df7-9218-43845a957d22",
   "metadata": {},
   "source": [
    "### 5.1 Calculate samples NMI mean and std per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f2f5a-ad38-434b-ae0c-43b3692ac093",
   "metadata": {},
   "source": [
    "#### In this code I load csv with samples and samples communities info and I group them per parameter and after that I calculate NMI mean and std\n",
    "#### At the end I store them in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5122641-b91f-4065-a83e-d1053aabff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff_original_wpp = pd.read_csv('ForestFire/communities/networks_whatsapp_samples_characterization.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5de537-04b6-498a-9cd6-09292aff6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_wpp = df_ff_original_wpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5961e-f3ba-4f84-b4e6-4f5548f4b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff_original_wpp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3380cd0-0ff4-4361-bf51-9a4675a788af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_wpp[\"nmi_mean\"]=0\n",
    "df_FF_NMI_wpp[\"std_1\"]=0\n",
    "df_FF_NMI_wpp[\"nmi_std\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dff7c-1ff6-4275-8114-83c118a092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_wpp['nmi_norm'] = df_FF_NMI_wpp['nmi'] / 0.4552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015536ab-67e8-46d9-b444-530155ad566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_wpp['nmi_mean'] = df_FF_NMI_wpp.groupby(['Net. Sample','Parameter'])['nmi_norm'].transform(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815da806-b927-4208-91ff-4ae70f07d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_wpp['std_1'] = df_FF_NMI_wpp.groupby(['Net. Sample','Parameter'])['nmi_norm'].transform(lambda x: x.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bd8e7-681a-402e-b94a-18e37cd83f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_wpp[\"nmi_std\"] = df_FF_NMI_wpp.apply(lambda x: '{:.4f}±{:.4f}'.format(x[\"nmi_mean\"], x[\"std_1\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9235d4-45c7-4dd1-a83b-313057122523",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = df_FF_NMI_wpp[['Batch','Net. Sample', 'Parameter','nmi_std']].copy()\n",
    "new_df_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0f0b7-0527-4f90-9c9b-d7324f133a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# new_df_2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb31ed-8354-45e8-8b4b-574a48e5ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df_2 = new_df_2[new_df_2['Batch'] == 1]\n",
    "rslt_df_2.drop('Batch', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266c199-7ea9-4a47-9738-81cf55c46738",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2 = rslt_df_2.pivot(index=[\"Net. Sample\"], columns=[\"Parameter\"])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pivoted_2[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d36cca-88f0-454f-aa9d-10d3661b30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:100].to_csv(\"ForestFire/communities/networks_whatsapp_samples_nmi_normalized.csv\", mode='a', header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76fc39-959f-4632-80bb-d523a64b1c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6 - Execute Random Walk Method to generate original network samples (we have generated 10 samples for each param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a250f-70e6-4b47-ab1a-2e9dca5bf378",
   "metadata": {},
   "source": [
    "#### Created relabel method to handle limitation of networkX\n",
    "#### Random Walk Reference: https://little-ball-of-fur.readthedocs.io/en/latest/_modules/littleballoffur/exploration_sampling/randomwalksampler.html#RandomWalkSampler\n",
    "#### I also stored samples to be able to detect its communities next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012eb98-4b36-40bf-9dc7-e73b653107b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_nodes(graph):\n",
    "    nodes = sorted(graph.nodes)\n",
    "    relabel_mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "    relabeled_graph = nx.relabel_nodes(graph, relabel_mapping)\n",
    "    \n",
    "    # Update node weights in the relabeled graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if u in relabel_mapping and v in relabel_mapping:\n",
    "            u_mapped = relabel_mapping[u]\n",
    "            v_mapped = relabel_mapping[v]\n",
    "            if u_mapped in relabeled_graph and v_mapped in relabeled_graph[u_mapped]:\n",
    "                relabeled_graph[u_mapped][v_mapped]['weight'] = data['weight']\n",
    "    \n",
    "    return relabeled_graph, relabel_mapping\n",
    "\n",
    "def revert_relabel_nodes(graph, relabel_mapping):\n",
    "    original_mapping = {v: k for k, v in relabel_mapping.items()}\n",
    "    reverted_graph = nx.relabel_nodes(graph, original_mapping)\n",
    "    return reverted_graph\n",
    "\n",
    "def run_random_walk_sampling_model(graph_nx, dataset):\n",
    "    j = 1\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Step 1: Apply relabel_nodes(graph) to weighted graph_nx\n",
    "    relabeled_graph_nx, relabel_mapping = relabel_nodes(graph_nx)\n",
    "    \n",
    "    while j <= 10:\n",
    "        print(j)\n",
    "        values_seed = []\n",
    "        seed = random.randint(0, 4294967294)\n",
    "        \n",
    "        for i in range(1, 10, 1):\n",
    "            random_node_model = RandomWalkSampler(number_of_nodes=int((i/10)*relabeled_graph_nx.number_of_nodes()), seed=seed)\n",
    "            \n",
    "            # Step 2: Use the weighted relabeled_graph to calculate sample\n",
    "            sample_random_node = random_node_model.sample(relabeled_graph_nx)\n",
    "            \n",
    "            # Step 3: Revert relabeling to original nodes mapped in relabel_mapping\n",
    "            relabeled_sample_random_node = revert_relabel_nodes(sample_random_node, relabel_mapping)\n",
    "            \n",
    "            result = graph_nx.subgraph(relabeled_sample_random_node.nodes()).edges() == relabeled_sample_random_node.edges()\n",
    "            print(result)\n",
    "                \n",
    "            df_random_node = nx.to_pandas_edgelist(relabeled_sample_random_node, source='source', target='target', nodelist=list(relabeled_sample_random_node.nodes), dtype=None, edge_key=None)\n",
    "\n",
    "            df_random_node.to_csv(\"whatsapp/RandomWalk/samples/\"+dataset+\"_\"+str(i/10)+'_random_walk_edge_list_batch_'+str(j)+'_.csv',index=False)\n",
    "            print(\"saved_\"+dataset+\"_\"+str(i/10)+'_random_walk_edge_list_batch_'+str(j)+'_'+str(seed)+'_.csv')\n",
    "            \n",
    "            values_seed.append(seed)\n",
    "        \n",
    "        df['batch_seed_'+str(j)] = values_seed\n",
    "        df.to_csv(\"whatsapp/RandomWalk/samples/whatsapp_seeds.csv\",index=False)\n",
    "        del seed\n",
    "        j += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b71d19-4e88-403f-90c8-ca208e804c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_random_walk_sampling_model(graph, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f501d9-75ad-4ccd-9d4a-4c0b685652c8",
   "metadata": {},
   "source": [
    "## 7. Detect community of original network generated samples - Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750f78c-9cf6-481a-9d65-3b579f2886fb",
   "metadata": {},
   "source": [
    "##### In this scenario I have created a separeted method called characterize_random_walk_sample_networks to call characterize_sampled_network to detect the  communities of each sample and, after that, I call in it a separated method calculate_nmi to calcutate NMI of the samples.\n",
    "##### I also store the communities info in a separated files for further analysis and I put all the samples info in another csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35966d36-5a58-4405-829f-84896c61529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nmi(dicionario1_pkl, dicionario2_pkl, min_tamanho_comunidade=10):\n",
    "    #from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "    # Carregar os dicionários a partir dos arquivos pickle\n",
    "    with open(dicionario1_pkl, 'rb') as arquivo1:\n",
    "        dict1 = pkl.load(arquivo1)\n",
    "    \n",
    "    with open(dicionario2_pkl, 'rb') as arquivo2:\n",
    "        dict2 = pkl.load(arquivo2)\n",
    "        \n",
    "    dicionario1 = {int(key): value for key, value in dict1.items()}\n",
    "    dicionario2 = {int(key): value for key, value in dict2.items()}\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "    \n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "    \n",
    "    chaves_comuns = list(set(comunidades_filtradas1.keys()).intersection(comunidades_filtradas2.keys()))\n",
    "    \n",
    "    intersection = len(chaves_comuns)\n",
    "    print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "    \n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "    \n",
    "    if(intersection <= 1): return 0, 0\n",
    "\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "    print(nmi)\n",
    "    \n",
    "    return nmi, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5842f71-560a-469f-bb38-9a53e218e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sampled_network(graph, Path_Communities, network, type_network, snap, s, parameter, header, sample, batch):\n",
    "    \n",
    "    #Para rede amostrada\n",
    "    list_results = []\n",
    "    print(graph)\n",
    "    df = pd.read_csv(graph)\n",
    "    Graphtype = nx.Graph()\n",
    "    G_sampled = nx.from_pandas_edgelist(df, edge_attr='weight', create_using=Graphtype)\n",
    "    \n",
    "    print(df[:3])\n",
    "    \n",
    "    # for component in list(nx.connected_components(G_sampled)):\n",
    "    #     if len(component)<3:\n",
    "    #         for node in component:\n",
    "    #             G_sampled.remove_node(node)\n",
    "                \n",
    "    n_nodes = len(G_sampled.nodes())\n",
    "    n_edges = len(G_sampled.edges())\n",
    "    \n",
    "    degree = dict(G_sampled.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G_sampled, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G_sampled),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G_sampled)\n",
    "    components = list(nx.connected_components(G_sampled))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G_sampled),4)\n",
    "    clustering_coeffs = nx.clustering(G_sampled)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    \n",
    "    complete_partition_sampled = community.best_partition(G_sampled, resolution=1)\n",
    "    modularity = community.modularity(complete_partition_sampled, G_sampled)\n",
    "    \n",
    "    pkl.dump(complete_partition_sampled, open(Path_Communities, \"wb\"), protocol=4) \n",
    "    \n",
    "    n_comm_10 = len(set(complete_partition_sampled.values()))\n",
    "    \n",
    "    community_sizes_10 = Counter(complete_partition_sampled.values())\n",
    "    \n",
    "    biggest_community_size_10 = max(community_sizes_10.values())\n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G_sampled)\n",
    "    \n",
    "    # Calcula o NMI entre os arquivos pickle\n",
    "    nmi, intersection = calculate_nmi('whatsapp/original/comm_complete_whatsapp_1.pkl', Path_Communities)\n",
    "    \n",
    "    parameter = parameter\n",
    "    sample = sample\n",
    "    batch = batch\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"sample_RW\"\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm_10,biggest_community_size_10, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "    del G_sampled\n",
    "    # del G\n",
    "    # del partition\n",
    "    del complete_partition_sampled\n",
    "\n",
    "    df.to_csv(\"whatsapp/RandomWalk/communities/networks_whatsapp_samples_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374688cc-14eb-4fc9-abf5-bdd81f6f487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_random_walk_sample_networks(graph, dataset):\n",
    "    header = True\n",
    "    j = 1\n",
    "    \n",
    "    while j <= 10:\n",
    "        print(j)\n",
    "        for i in range(1,10,1):\n",
    "            sample = \"whatsapp/RandomWalk/samples/\"+dataset+\"_\"+str(i/10)+'_random_walk_edge_list_batch_'+str(j)+'_.csv'\n",
    "            Path_Communities = \"whatsapp/RandomWalk/communities/\"+dataset+\"_RW_batch_\"+str(j)+\"_net_percent_\"+str(i/10)+\".pkl\"\n",
    "            characterize_sampled_network(sample, Path_Communities, dataset, \"sample whatsapp - Random Walk\", \"sample_\"+str(i)+\"_batch_\"+str(j), 1, 0, header, str(i/10), str(j))\n",
    "            # print(\"saved_community_\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv')\n",
    "            header = False\n",
    "        header = False\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d04238-b2f1-48df-9c04-c4a6c6285a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characterize_random_walk_sample_networks(graph, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a885c4-9aeb-42e2-a84c-474303591f98",
   "metadata": {},
   "source": [
    "### 7.1 Calculate samples NMI mean and std per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce91edb-2bb8-4e3b-8bc3-1bfa935a3a64",
   "metadata": {},
   "source": [
    "#### In this code I load csv with samples and samples communities info and I group them per parameter and after that I calculate NMI mean and std\n",
    "#### At the end I store them in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3176ff5-1a7c-495d-9f4f-e882713d8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_walk_original_whatsapp = pd.read_csv('RandomWalk/communities/networks_whatsapp_samples_characterization.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fad98-5a0c-4efb-b6bb-94f64e0adffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp = df_random_walk_original_whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f2767-62a9-4918-a23d-27d96ca80ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22ff1e-59ac-445f-b69f-ac0b28531f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp[\"nmi_mean\"]=0\n",
    "df_RW_NMI_whatsapp[\"std\"]=0\n",
    "df_RW_NMI_whatsapp[\"nmi_std\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d4dc7-0259-43c8-bdd8-033ac861e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp['nmi_norm'] = df_RW_NMI_whatsapp['nmi'] / 0.4552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2f5d3-f815-42c0-b59b-7401248e660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp['nmi_mean'] = df_RW_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbceaa8-7785-4bfc-ae71-56a9f3d93879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp['std'] = df_RW_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31946f13-b2c8-407d-bbc9-b424d9ec80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RW_NMI_whatsapp[\"nmi_std\"] = df_RW_NMI_whatsapp.apply(lambda x: '{:.2f}±{:.2f}'.format(x[\"nmi_mean\"], x[\"std\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3edbb-97a3-4bd3-a0e1-c682bd8fe2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = df_RW_NMI_whatsapp[['Batch', 'Net. Sample','nmi_std']].copy()\n",
    "new_df_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4a4f2-d5b7-4c64-b47c-f37e8c3fb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2 = new_df_2.pivot(index=[\"Batch\"], columns=\"Net. Sample\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pivoted_2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce6167-a6ed-4d0f-b974-ae0d95923eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089e737-2e5a-4a29-99a4-623c4a8dc921",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1].to_csv(\"RandomWalk/communities/networks_nmi_whatsapp_samples_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834d14a-e3f5-4ec4-b1b4-90158d00000f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5813c186-93f9-4b59-b219-46e94d556926",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe72b2-124f-4092-93cf-bb86cfee0ca6",
   "metadata": {},
   "source": [
    "#### In this section we handle backbone sampling and community detection from our proposed methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588a5f8-6d1f-43a2-b528-2035cd9c3ed7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load original network and calculate its backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61f156-dfff-4fd0-9bce-f99f8c4036db",
   "metadata": {},
   "source": [
    "### In our methodology we use two backbone extraction tequiniques according to the dataset reference: Disparity Filter or Gloss Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b9f0b-9089-4e2b-a4d8-80645df62519",
   "metadata": {},
   "source": [
    "### 1.1 Disparity Filter\n",
    "\n",
    "##### In this section we present disparity filter used in our approach\n",
    "##### After apply it in an original network we store the backbone in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300ebf2-c41f-400e-8a53-f9845d33edae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def disparity_filter(table, undirected = True, return_self_loops = True):\n",
    "    sys.stderr.write(\"Calculating DF score...\\n\")\n",
    "    table = table.copy()\n",
    "    table.reset_index(inplace=True)\n",
    "    table_sum = table.groupby(table[\"source\"]).sum().reset_index()\n",
    "    table_deg = table.groupby(table[\"source\"]).count()[\"target\"].reset_index()\n",
    "    table = table.merge(table_sum, on = \"source\", how = \"left\", suffixes = (\"\", \"_sum\"))\n",
    "    table = table.merge(table_deg, on = \"source\", how = \"left\", suffixes = (\"\", \"_count\"))\n",
    "    table[\"score\"] = 1.0 - ((1.0 - (table[\"weight\"] / table[\"weight_sum\"])) ** (table[\"target_count\"] - 1))\n",
    "    table[\"variance\"] = (table[\"target_count\"] ** 2) * (((20 + (4.0 * table[\"target_count\"])) / ((table[\"target_count\"] + 1.0) * (table[\"target_count\"] + 2) * (table[\"target_count\"] + 3))) - ((4.0) / ((table[\"target_count\"] + 1.0) ** 2)))\n",
    "    if not return_self_loops:\n",
    "        table = table[table[\"source\"] != table[\"target\"]]\n",
    "    if undirected:\n",
    "        table[\"edge\"] = table.apply(lambda x: \"%s-%s\" % (min(x[\"source\"], x[\"target\"]), max(x[\"source\"], x[\"target\"])), axis = 1)\n",
    "        table_maxscore = table.groupby(by = \"edge\")[\"score\"].max().reset_index()\n",
    "        table_minvar = table.groupby(by = \"edge\")[\"variance\"].min().reset_index()\n",
    "        table = table.merge(table_maxscore, on = \"edge\", suffixes = (\"_min\", \"\"))\n",
    "        table = table.merge(table_minvar, on = \"edge\", suffixes = (\"_max\", \"\"))\n",
    "        table = table.drop_duplicates(subset = [\"edge\"])\n",
    "        table = table.drop(\"edge\", axis=1)\n",
    "        table = table.drop(\"score_min\", axis=1)\n",
    "        table = table.drop(\"variance_max\", axis=1)\n",
    "    return table[[\"source\", \"target\", \"weight\", \"score\", \"variance\"]]\n",
    "\n",
    "\n",
    "def read(filename, column_of_interest, triangular_input = False, \n",
    "         consider_self_loops = True, undirected = True, drop_zeroes = True, sep = \",\"):\n",
    "\n",
    "    table = pd.read_csv(filename, sep = sep)\n",
    "    table = table[[\"source\", \"target\", column_of_interest]]\n",
    "    table.rename(columns = {column_of_interest: \"weight\"}, inplace = True)\n",
    "    if drop_zeroes:\n",
    "        table = table[table[\"weight\"] > 0]\n",
    "    if not consider_self_loops:\n",
    "        table = table[table[\"source\"] != table[\"target\"]]\n",
    "    if triangular_input:\n",
    "        table2 = table.copy()\n",
    "        table2[\"new_source\"] = table[\"target\"]\n",
    "        table2[\"new_target\"] = table[\"source\"]\n",
    "        table2.drop(\"source\", axis=1, inplace = True)\n",
    "        table2.drop(\"target\", axis=1, inplace = True)\n",
    "        table2 = table2.rename(columns = {\"new_source\": \"source\", \"new_target\": \"target\"})\n",
    "        table = pd.concat([table, table2], axis = 0)\n",
    "        table = table.drop_duplicates(subset = [\"source\", \"target\"])\n",
    "    original_nodes = len(set(table[\"source\"]) | set(table[\"target\"]))\n",
    "    original_edges = table.shape[0]\n",
    "    if undirected:\n",
    "        return table\n",
    "    else:\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d783ed5-2e2b-401f-adf1-15e0d15c3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = read('OctoberWpp.edgelist', column_of_interest='weight',\n",
    "                triangular_input=True,\n",
    "                consider_self_loops=False,\n",
    "                undirected=True, drop_zeroes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f3813-182f-45a5-9301-61d691f9dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d94e8c-1dea-4cac-9797-2abbe5276e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.90\n",
    "nc_table = disparity_filter(df_edges, undirected = True)\n",
    "nc_table = nc_table[nc_table['score'] >= confidence]\n",
    "print(nc_table[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be782da-719b-483f-9153-6acc614928c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_file_csv = nc_table.iloc[:, :3]\n",
    "backbone_file_csv = backbone_file_csv.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee6947-5535-4f76-a95e-64bc82c801a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_file_csv[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6891c-a13e-4e07-b90b-2443489b5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_table = nc_table.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41721c01-f129-4153-a675-6c0bbd98f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_table[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33da71-3f09-4c02-a7ec-f78e342ff663",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_table.to_csv(\"whatsapp/backbone/backbone_complete.csv\",index=False)\n",
    "backbone_file_csv.to_csv(\"whatsapp/backbone/backbone_wpp.edgelist\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e595971-99c6-4729-b84b-60a082f1a9cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Calculate the backbone of each sample and store it in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f862020-ca52-4960-ba09-b0b886a3ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backbone_random_node_samples(num_repeticoes, amostragem):\n",
    "    dataset = 'whatsapp'\n",
    "    excluded_numbers = []\n",
    "    j = 1\n",
    "    for _ in range(num_repeticoes):\n",
    "        \n",
    "        file = \"whatsapp/RandomNode/samples/\"+dataset+\"_\"+str(amostragem)+'_random_node_edge_list_batch_'+str(j)+'_.csv'\n",
    "        df_edges = read(file, column_of_interest='weight',\n",
    "                triangular_input=True,\n",
    "                consider_self_loops=False,\n",
    "                undirected=True, drop_zeroes=True)\n",
    "\n",
    "        nc_table = disparity_filter(df_edges)\n",
    "        nc_table = nc_table.reset_index(drop=True)\n",
    "        nc_table.to_csv(\"whatsapp/RandomNode/backbone_samples_complete/\"+dataset+\"_\"+str(amostragem)+'_random_node_edge_list_batch_'+str(j)+'_.csv',index=False)\n",
    "        \n",
    "        \n",
    "        print(nc_table[:3])\n",
    "        \n",
    "        \n",
    "        sample = nc_table.iloc[:, :3]\n",
    "        print(sample[:3])\n",
    "        sample= sample.reset_index(drop=True)\n",
    "        sample.to_csv(\"whatsapp/RandomNode/backbone_samples/\"+dataset+\"_\"+str(amostragem)+'_random_node_edge_list_batch_'+str(j)+'_.csv',index=False)\n",
    "\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338834a-1f04-4641-bddd-72692145fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backbone_random_walk_samples(num_repeticoes, amostragem):\n",
    "    dataset = 'whatsapp'\n",
    "    excluded_numbers = []\n",
    "    j = 1\n",
    "    for _ in range(num_repeticoes):\n",
    "        \n",
    "        file = \"whatsapp/RandomWalk/samples/\"+dataset+\"_\"+str(amostragem)+'_random_walk_edge_list_batch_'+str(j)+'_.csv'\n",
    "        df_edges = read(file, column_of_interest='weight',\n",
    "                triangular_input=True,\n",
    "                consider_self_loops=False,\n",
    "                undirected=True, drop_zeroes=True)\n",
    "\n",
    "        nc_table = disparity_filter(df_edges)\n",
    "        nc_table = nc_table.reset_index(drop=True)\n",
    "        nc_table.to_csv(\"whatsapp/RandomWalk/backbone_samples_complete/\"+dataset+\"_\"+str(amostragem)+'_random_walk_edge_list_batch_'+str(j)+'_.csv',index=False)\n",
    "        \n",
    "        \n",
    "        print(nc_table[:3])\n",
    "        \n",
    "        \n",
    "        sample = nc_table.iloc[:, :3]\n",
    "        print(sample[:3])\n",
    "        sample= sample.reset_index(drop=True)\n",
    "        sample.to_csv(\"whatsapp/RandomWalk/backbone_samples/\"+dataset+\"_\"+str(amostragem)+'_random_walk_edge_list_batch_'+str(j)+'_.csv',index=False)\n",
    "\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586248dc-51ef-4ed7-aa07-e1eaf0ef8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backbone_forest_fire_samples():\n",
    "    # Inicializa uma lista para armazenar os resultados do NMI\n",
    "    dataset = 'whatsapp'\n",
    "    excluded_numbers = []\n",
    "    k = 1\n",
    "    # Realiza a amostragem e o cálculo do NMI várias vezes\n",
    "    while k <= 10:\n",
    "        print(k)\n",
    "        for i in range(1,11,1):\n",
    "            values_seed = []\n",
    "            for j in range(1,10,1):\n",
    "                file = \"whatsapp/ForestFire/samples/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv'\n",
    "                df_edges = read(file, column_of_interest='weight',\n",
    "                triangular_input=True,\n",
    "                consider_self_loops=False,\n",
    "                undirected=True, drop_zeroes=True)\n",
    "                nc_table = disparity_filter(df_edges)\n",
    "                nc_table = nc_table.reset_index(drop=True)\n",
    "                nc_table.to_csv(\"whatsapp/ForestFire/backbone_samples_complete/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv',index=False)\n",
    "                sample = nc_table.iloc[:, :3]\n",
    "                sample.to_csv(\"whatsapp/ForestFire/backbone_samples/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv',index=False)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec266f-84b0-4b28-b3c0-837f3ce69f58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Execute 10x community detection in backbone of the original network\n",
    "\n",
    "##### I store in a csv to have a backup for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f4aeb-1b79-421f-93f6-e7d6ec3a9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_path = 'whatsapp/backbone/backbone_wpp.edgelist'\n",
    "G = nx.read_weighted_edgelist(edgelist_path, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6364d-ea31-4383-b819-a879980d3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = G\n",
    "number_of_nodes = graph.number_of_nodes()\n",
    "print(number_of_nodes)\n",
    "print(graph.number_of_edges())\n",
    "print(graph.is_directed())\n",
    "# print(nx.is_connected(graph))\n",
    "is_connected = nx.is_connected(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944b2fe-6215-4c35-893d-5f21a0555ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_network_executions(G, Path_Communities, header,index):\n",
    "    list_results = []\n",
    "                \n",
    "    n_nodes = len(G.nodes())\n",
    "    n_edges = len(G.edges())\n",
    "    \n",
    "    degree = dict(G.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G)\n",
    "    components = list(nx.connected_components(G))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G),4)\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    # Calculate Louvain community detection\n",
    "    partition = community.best_partition(G, resolution=1)\n",
    "    modularity = community.modularity(partition, G)\n",
    "    \n",
    "    partition = {node: comm_id for node, comm_id in partition.items() if list(partition.values()).count(comm_id) > 10}\n",
    "    \n",
    "    pkl.dump(partition, open(Path_Communities, \"wb\"), protocol=4)\n",
    "    \n",
    "    n_comm = len(set(partition.values()))\n",
    "    community_sizes = Counter(partition.values())\n",
    "    biggest_community_size = max(community_sizes.values())\n",
    "    \n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G)\n",
    "    \n",
    "    parameter = '-'\n",
    "    sample = \"-\"\n",
    "    batch = \"-\"\n",
    "    intersection = \"-\"\n",
    "    nmi = \"-\"\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"original_\"+str(i)\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm,biggest_community_size, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "\n",
    "    df.to_csv(\"whatsapp/backbone/networks_whatsapp_original_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a3a31-dbc2-4908-b8dd-d72f4b43cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = True\n",
    "for i in range(1, 11):\n",
    "    characterize_network_executions(graph, 'whatsapp/backbone/comm_complete_whatsapp_'+str(i)+'.pkl', header, i)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56322d00-3087-4051-803b-3016957394a7",
   "metadata": {},
   "source": [
    "### 3.1 Compare communities of the executions and calculate NMI mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b76842-3675-41ca-8fec-94a33a82c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tamanho_comunidade = 10\n",
    "PATH_Communities = 'whatsapp/backbone/'\n",
    "list_results = []\n",
    "list_models = ['comm_complete_whatsapp_1', 'comm_complete_whatsapp_2', \n",
    "               'comm_complete_whatsapp_3', 'comm_complete_whatsapp_4', \n",
    "               'comm_complete_whatsapp_5', 'comm_complete_whatsapp_6', \n",
    "               'comm_complete_whatsapp_7', 'comm_complete_whatsapp_8',\n",
    "               'comm_complete_whatsapp_9', 'comm_complete_whatsapp_10']\n",
    "\n",
    "for i in range(1,len(list_models),1):\n",
    "    model_1 = list_models[0]\n",
    "    model_2 = list_models[i]\n",
    "  \n",
    "    # Load the pickled communities\n",
    "    with open(PATH_Communities + model_1 + '.pkl', 'rb') as file:\n",
    "        dicionario1 = pickle.load(file)\n",
    "    with open(PATH_Communities + model_2 + '.pkl', 'rb') as file:\n",
    "        dicionario2 = pickle.load(file)\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "\n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "\n",
    "    # Obter as chaves em comum entre os dicionários filtrados\n",
    "    chaves_comuns = set(comunidades_filtradas1.keys()) & set(comunidades_filtradas2.keys())\n",
    "    intersection = len(chaves_comuns)\n",
    "    # print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "\n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "\n",
    "    nmi = 0\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    if(intersection> 1):\n",
    "        nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "\n",
    "    list_results.append((model_1, model_2, nmi))\n",
    "\n",
    "df = pd.DataFrame(list_results, columns = ['Model 1', 'Model 2', 'NMI'])\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "csv_file = 'whatsapp/backbone/nmi_original_whatsapp_results.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "df\n",
    "\n",
    "mean_nmi = df[\"NMI\"].mean()\n",
    "std_nmi = df[\"NMI\"].std()\n",
    "\n",
    "# Create a new DataFrame with the mean and std values\n",
    "result_df = pd.DataFrame({\"NMI\": ['{:.4f}±{:.4f}'.format(mean_nmi,std_nmi)]})\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "csv_file = 'whatsapp/backbone/nmi_mean_whatsapp_results.csv'\n",
    "result_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3b0e7-3ccf-4893-80d0-b41e3f9d3ee2",
   "metadata": {},
   "source": [
    "## 4. Detect community of backbones of the generated samples networks - Random Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220eeb5a-d95e-4e7b-abb7-1c905c6e91eb",
   "metadata": {},
   "source": [
    "##### In this scenario I have created a separeted method called characterize_random_node_sample_networks_backbone_rn to call characterize_sampled_network_backbone_rn to detect the  communities of each sample and, after that, I call in it a separated method calculate_nmi to calcutate NMI of the samples.\n",
    "##### I also store the communities info in a separated files for further analysis and I put all the backbone samples info in another csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11609b7b-0eea-4542-a565-6f8d20964412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nmi(dicionario1_pkl, dicionario2_pkl, min_tamanho_comunidade=10):\n",
    "    #from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "    # Carregar os dicionários a partir dos arquivos pickle\n",
    "    with open(dicionario1_pkl, 'rb') as arquivo1:\n",
    "        dict1 = pkl.load(arquivo1)\n",
    "    \n",
    "    with open(dicionario2_pkl, 'rb') as arquivo2:\n",
    "        dict2 = pkl.load(arquivo2)\n",
    "        \n",
    "    dicionario1 = {int(key): value for key, value in dict1.items()}\n",
    "    dicionario2 = {int(key): value for key, value in dict2.items()}\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "    \n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "    \n",
    "    chaves_comuns = list(set(comunidades_filtradas1.keys()).intersection(comunidades_filtradas2.keys()))\n",
    "    \n",
    "    intersection = len(chaves_comuns)\n",
    "    print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "    \n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "    \n",
    "    if(intersection <= 1): return 0, 0\n",
    "\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "    print(nmi)\n",
    "    \n",
    "    return nmi, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9ac83-00dd-44ef-8ece-b156ce6df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sampled_network_backbone_rn(graph, Path_Communities, network, type_network, snap, s, parameter, header, sample, batch):\n",
    "    #Para rede amostrada\n",
    "    list_results = []\n",
    "    print(graph)\n",
    "    original_df = pd.read_csv(graph)\n",
    "    print(original_df[:3])\n",
    "    filtered_df = original_df[original_df['score'] > 0.90]\n",
    "    print(filtered_df[:3])\n",
    "    df = filtered_df.iloc[:, :3]\n",
    "    print(df[:3])\n",
    "    Graphtype = nx.Graph()\n",
    "    G_sampled = nx.from_pandas_edgelist(df, edge_attr='weight', create_using=Graphtype)\n",
    "    \n",
    "    # for component in list(nx.connected_components(G_sampled)):\n",
    "    #     if len(component)<3:\n",
    "    #         for node in component:\n",
    "    #             G_sampled.remove_node(node)\n",
    "                \n",
    "    n_nodes = len(G_sampled.nodes())\n",
    "    n_edges = len(G_sampled.edges())\n",
    "    \n",
    "    degree = dict(G_sampled.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G_sampled, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G_sampled),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G_sampled)\n",
    "    components = list(nx.connected_components(G_sampled))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G_sampled),4)\n",
    "    clustering_coeffs = nx.clustering(G_sampled)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    \n",
    "    complete_partition_sampled = community.best_partition(G_sampled, resolution=1)\n",
    "    modularity = community.modularity(complete_partition_sampled, G_sampled)\n",
    "    \n",
    "    pkl.dump(complete_partition_sampled, open(Path_Communities, \"wb\"), protocol=4) \n",
    "    \n",
    "    n_comm_10 = len(set(complete_partition_sampled.values()))\n",
    "    \n",
    "    community_sizes_10 = Counter(complete_partition_sampled.values())\n",
    "    \n",
    "    biggest_community_size_10 = max(community_sizes_10.values())\n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G_sampled)\n",
    "    \n",
    "    # Calcula o NMI entre os arquivos pickle\n",
    "    nmi, intersection = calculate_nmi('whatsapp/backbone/comm_complete_whatsapp_1.pkl', Path_Communities)\n",
    "    \n",
    "    parameter = parameter\n",
    "    sample = sample\n",
    "    batch = batch\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"sample_RN\"\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm_10,biggest_community_size_10, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "    del G_sampled\n",
    "    # del G\n",
    "    # del partition\n",
    "    del complete_partition_sampled\n",
    "\n",
    "    df.to_csv(\"whatsapp/RandomNode/communities_backbone/networks_whatsapp_samples_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016da288-49b3-4d1f-afda-bc539fd0eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_random_node_sample_networks_backbone_rn(graph, dataset):\n",
    "    header = True\n",
    "    j = 1\n",
    "    \n",
    "    while j <= 10:\n",
    "        print(j)\n",
    "        for i in range(1,11,1):\n",
    "            sample = \"whatsapp/RandomNode/backbone_samples_complete/\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv'\n",
    "            Path_Communities = \"whatsapp/RandomNode/communities_backbone/\"+dataset+\"_RN_batch_\"+str(j)+\"_net_percent_\"+str(i/10)+\".pkl\"\n",
    "            characterize_sampled_network_backbone_rn(sample, Path_Communities, dataset, \"sample whatsapp - Random Node\", \"sample_\"+str(i)+\"_batch_\"+str(j), 1, 0, header, str(i/10), str(j))\n",
    "            # print(\"saved_community_\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv')\n",
    "            header = False\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a03047-b1d3-4b0d-83aa-902d65f9f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characterize_random_node_sample_networks_backbone_rn(graph, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c921b-1305-498b-aef7-e353cdd2e87e",
   "metadata": {},
   "source": [
    "### 4.1 Calculate samples NMI mean and std per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77373fd6-b34e-4fec-a9bf-2d460873de5a",
   "metadata": {},
   "source": [
    "#### In this code I load csv with samples and samples communities info and I group them per parameter and after that I calculate NMI mean and std\n",
    "#### At the end I store them in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabea01b-7c5c-474d-938e-7895c62bac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_random_node_original_whatsapp, df_RN_NMI_whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210807b5-8540-4606-abc1-bec49301bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_node_original_whatsapp = pd.read_csv('RandomNode/communities_backbone/networks_whatsapp_samples_characterization.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a19101-e761-40e9-8ecb-529979f4b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp = df_random_node_original_whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85bf69-853e-4d3e-bc35-2f80789cb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe038ddb-be8c-4e8f-8e25-27a50ab3afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[\"nmi_mean\"]=0\n",
    "df_RN_NMI_whatsapp[\"std\"]=0\n",
    "df_RN_NMI_whatsapp[\"nmi_std\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c40d0-7036-436c-bd60-88b1030068b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['nmi_norm'] = df_RN_NMI_whatsapp['nmi'] / 0.5875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc0346-b2a5-401b-a139-21d08ebb1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['nmi_mean'] = df_RN_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58694b-f01a-4658-83ba-26ce16b39e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['std'] = df_RN_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1600bf-b860-4276-b5c9-5fa2aaefa56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[\"nmi_std\"] = df_RN_NMI_whatsapp.apply(lambda x: '{:.2f}±{:.2f}'.format(x[\"nmi_mean\"], x[\"std\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89095912-5e82-4827-9a5e-f7d76e9e8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = df_RN_NMI_whatsapp[['Batch', 'Net. Sample','nmi_std']].copy()\n",
    "new_df_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcea2b-6319-4eec-b2e8-5b422b64c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2 = new_df_2.pivot(index=[\"Batch\"], columns=\"Net. Sample\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pivoted_2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a30ac-8b85-4ef1-b711-4d1e5d436f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3f152-ead5-4f92-9236-77b0e3c0e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1].to_csv(\"RandomNode/communities_backbone/networks_nmi_whatsapp_samples_normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcb74d-de69-41f0-a990-490a97b6fef1",
   "metadata": {},
   "source": [
    "## 5. Detect community of backbones of the generated samples networks - Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc5e11-b3c5-43b7-bb9a-ec5055d06657",
   "metadata": {},
   "source": [
    "##### In this scenario I have created a separeted method called characterize_random_node_sample_networks_backbone_rw to call characterize_sampled_network_backbone_rw to detect the  communities of each sample and, after that, I call in it a separated method calculate_nmi to calcutate NMI of the samples.\n",
    "##### I also store the communities info in a separated files for further analysis and I put all the backbone samples info in another csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53852aee-e2af-4f43-b492-99592d56e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nmi(dicionario1_pkl, dicionario2_pkl, min_tamanho_comunidade=10):\n",
    "    #from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "    # Carregar os dicionários a partir dos arquivos pickle\n",
    "    with open(dicionario1_pkl, 'rb') as arquivo1:\n",
    "        dict1 = pkl.load(arquivo1)\n",
    "    \n",
    "    with open(dicionario2_pkl, 'rb') as arquivo2:\n",
    "        dict2 = pkl.load(arquivo2)\n",
    "        \n",
    "    dicionario1 = {int(key): value for key, value in dict1.items()}\n",
    "    dicionario2 = {int(key): value for key, value in dict2.items()}\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "    \n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "    \n",
    "    chaves_comuns = list(set(comunidades_filtradas1.keys()).intersection(comunidades_filtradas2.keys()))\n",
    "    \n",
    "    intersection = len(chaves_comuns)\n",
    "    print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "    \n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "    \n",
    "    if(intersection <= 1): return 0, 0\n",
    "\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "    print(nmi)\n",
    "    \n",
    "    return nmi, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f9195-f0d8-4bc0-b661-61b91a9a012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sampled_network_backbone_rw(graph, Path_Communities, network, type_network, snap, s, parameter, header, sample, batch):\n",
    "    #Para rede amostrada\n",
    "    list_results = []\n",
    "    print(graph)\n",
    "    original_df = pd.read_csv(graph)\n",
    "    print(original_df[:3])\n",
    "    filtered_df = original_df[original_df['score'] > 0.90]\n",
    "    print(filtered_df[:3])\n",
    "    df = filtered_df.iloc[:, :3]\n",
    "    print(df[:3])\n",
    "    Graphtype = nx.Graph()\n",
    "    G_sampled = nx.from_pandas_edgelist(df, edge_attr='weight', create_using=Graphtype)\n",
    "    \n",
    "    # for component in list(nx.connected_components(G_sampled)):\n",
    "    #     if len(component)<3:\n",
    "    #         for node in component:\n",
    "    #             G_sampled.remove_node(node)\n",
    "                \n",
    "    n_nodes = len(G_sampled.nodes())\n",
    "    n_edges = len(G_sampled.edges())\n",
    "    \n",
    "    degree = dict(G_sampled.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G_sampled, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G_sampled),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G_sampled)\n",
    "    components = list(nx.connected_components(G_sampled))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G_sampled),4)\n",
    "    clustering_coeffs = nx.clustering(G_sampled)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    \n",
    "    complete_partition_sampled = community.best_partition(G_sampled, resolution=1)\n",
    "    modularity = community.modularity(complete_partition_sampled, G_sampled)\n",
    "    \n",
    "    pkl.dump(complete_partition_sampled, open(Path_Communities, \"wb\"), protocol=4) \n",
    "    \n",
    "    n_comm_10 = len(set(complete_partition_sampled.values()))\n",
    "    \n",
    "    community_sizes_10 = Counter(complete_partition_sampled.values())\n",
    "    \n",
    "    biggest_community_size_10 = max(community_sizes_10.values())\n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G_sampled)\n",
    "    \n",
    "    # Calcula o NMI entre os arquivos pickle\n",
    "    nmi, intersection = calculate_nmi('whatsapp/backbone/comm_complete_whatsapp_1.pkl', Path_Communities)\n",
    "    \n",
    "    parameter = parameter\n",
    "    sample = sample\n",
    "    batch = batch\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"sample_RW\"\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm_10,biggest_community_size_10, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "    del G_sampled\n",
    "    # del G\n",
    "    # del partition\n",
    "    del complete_partition_sampled\n",
    "\n",
    "    df.to_csv(\"whatsapp/RandomWalk/communities_backbone/networks_whatsapp_samples_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe58cf-d11b-406f-8453-36764fc0d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_random_node_sample_networks_backbone_rw(graph, dataset):\n",
    "    header = True\n",
    "    j = 1\n",
    "    \n",
    "    while j <= 10:\n",
    "        print(j)\n",
    "        for i in range(1,10,1):\n",
    "            sample = \"whatsapp/RandomWalk/backbone_samples_complete/\"+dataset+\"_\"+str(i/10)+'_random_walk_edge_list_batch_'+str(j)+'_.csv'\n",
    "            Path_Communities = \"whatsapp/RandomWalk/communities_backbone/\"+dataset+\"_RW_batch_\"+str(j)+\"_net_percent_\"+str(i/10)+\".pkl\"\n",
    "            characterize_sampled_network_backbone_rw(sample, Path_Communities, dataset, \"sample whatsapp - Random Walk\", \"sample_\"+str(i)+\"_batch_\"+str(j), 1, 0, header, str(i/10), str(j))\n",
    "            # print(\"saved_community_\"+dataset+\"_\"+str(i/10)+'_random_node_edge_list_batch_'+str(j)+'_.csv')\n",
    "            header = False\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f202f0-5569-40c9-9160-2ef0a16ecb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characterize_random_node_sample_networks_backbone_rw(graph, 'whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595d365-013a-44b3-8f72-2bdb83326eb8",
   "metadata": {},
   "source": [
    "### 5.1 Calculate samples NMI mean and std per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1120d2a-ad4e-48b4-b1b7-8708a7c748a6",
   "metadata": {},
   "source": [
    "#### In this code I load csv with samples and samples communities info and I group them per parameter and after that I calculate NMI mean and std\n",
    "#### At the end I store them in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412022b7-fc97-448c-bceb-f83b313ea0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_node_original_whatsapp = pd.read_csv('RandomWalk/communities_backbone/networks_whatsapp_samples_characterization.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f346073-31bc-4483-96e1-de8f0d963394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp = df_random_node_original_whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31be4d8-68ef-4ff8-86ef-7f0cc3607bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7627d-12ab-4769-b7a4-5d086893dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[\"nmi_mean\"]=0\n",
    "df_RN_NMI_whatsapp[\"std\"]=0\n",
    "df_RN_NMI_whatsapp[\"nmi_std\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de0a8f-01e4-42c0-9322-afd8f85575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['nmi_norm'] = df_RN_NMI_whatsapp['nmi'] / 0.5875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1e246-b082-435d-b752-b6ecdcb0ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['nmi_mean'] = df_RN_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac58ff-2f51-4b86-a159-68d8265e7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp['std'] = df_RN_NMI_whatsapp.groupby(['Net. Sample'])['nmi_norm'].transform(lambda x: x.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564ad5b-8cdf-443e-a702-9623d0d4b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RN_NMI_whatsapp[\"nmi_std\"] = df_RN_NMI_whatsapp.apply(lambda x: '{:.2f}±{:.2f}'.format(x[\"nmi_mean\"], x[\"std\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84152cdf-f3e2-4cfb-9f26-80602bd6316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = df_RN_NMI_whatsapp[['Batch', 'Net. Sample','nmi_std']].copy()\n",
    "new_df_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594a02e-0ef5-42af-abfa-97497f6f68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2 = new_df_2.pivot(index=[\"Batch\"], columns=\"Net. Sample\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pivoted_2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9601b-6ac6-41f6-a34d-cf3a69a2dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba40106-c999-444a-8ff0-a266cf9eb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:1].to_csv(\"RandomWalk/communities_backbone/networks_nmi_whatsapp_samples_backbone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3a19c-15af-4497-8b7f-b10671291904",
   "metadata": {},
   "source": [
    "## 6. Detect community of backbones of the generated samples networks - Forest Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18cdf8-b375-4da3-9950-fe1e69cc048b",
   "metadata": {},
   "source": [
    "##### In this scenario I have created a separeted method called characterize_forest_fire_sample_networks_backbone to call characterize_sampled_network_backbone_ff to detect the  communities of each sample and, after that, I call in it a separated method calculate_nmi to calcutate NMI of the samples.\n",
    "##### I also store the communities info in a separated files for further analysis and I put all the backbone samples info in another csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe99ae7-4970-42c7-8287-96dabba458cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nmi(dicionario1_pkl, dicionario2_pkl, min_tamanho_comunidade=10):\n",
    "    #from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "    # Carregar os dicionários a partir dos arquivos pickle\n",
    "    with open(dicionario1_pkl, 'rb') as arquivo1:\n",
    "        dict1 = pkl.load(arquivo1)\n",
    "    \n",
    "    with open(dicionario2_pkl, 'rb') as arquivo2:\n",
    "        dict2 = pkl.load(arquivo2)\n",
    "        \n",
    "    dicionario1 = {int(key): value for key, value in dict1.items()}\n",
    "    dicionario2 = {int(key): value for key, value in dict2.items()}\n",
    "\n",
    "    # Contagem do tamanho das comunidades e filtragem\n",
    "    comunidades_filtradas1 = {}\n",
    "    comunidades_filtradas2 = {}\n",
    "    \n",
    "    # Filtra as comunidades com tamanho maior ou igual a min_tamanho_comunidade\n",
    "    for node, community_id in dicionario1.items():\n",
    "        tamanho_comunidade = list(dicionario1.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas1[node] = community_id\n",
    "\n",
    "    for node, community_id in dicionario2.items():\n",
    "        tamanho_comunidade = list(dicionario2.values()).count(community_id)\n",
    "        if tamanho_comunidade >= min_tamanho_comunidade:\n",
    "            comunidades_filtradas2[node] = community_id\n",
    "    \n",
    "    chaves_comuns = list(set(comunidades_filtradas1.keys()).intersection(comunidades_filtradas2.keys()))\n",
    "    \n",
    "    intersection = len(chaves_comuns)\n",
    "    print(intersection)\n",
    "    # Inicializar as listas para armazenar os valores correspondentes\n",
    "    valores_dicionario1 = []\n",
    "    valores_dicionario2 = []\n",
    "    \n",
    "    # Preencher as listas com os valores correspondentes, mantendo a ordem\n",
    "    for chave in chaves_comuns:\n",
    "        valores_dicionario1.append(comunidades_filtradas1[chave])\n",
    "        valores_dicionario2.append(comunidades_filtradas2[chave])\n",
    "    \n",
    "    if(intersection <= 1): return 0, 0\n",
    "\n",
    "    # Calcular o NMI usando scikit-learn\n",
    "    nmi = normalized_mutual_info_score(valores_dicionario1, valores_dicionario2)\n",
    "    print(nmi)\n",
    "    \n",
    "    return nmi, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a726ac-59d3-40c1-8b19-44f98d5091e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_sampled_network_backbone_ff(graph, Path_Communities, network, type_network, snap, s, parameter, header, sample, batch):\n",
    "    #Para rede amostrada\n",
    "    list_results = []\n",
    "    print(graph)\n",
    "    original_df = pd.read_csv(graph)\n",
    "    print(original_df[:3])\n",
    "    filtered_df = original_df[original_df['score'] > 0.90]\n",
    "    print(filtered_df[:3])\n",
    "    df = filtered_df.iloc[:, :3]\n",
    "    print(df[:3])\n",
    "    Graphtype = nx.Graph()\n",
    "    G_sampled = nx.from_pandas_edgelist(df, edge_attr='weight', create_using=Graphtype)\n",
    "    \n",
    "    # for component in list(nx.connected_components(G_sampled)):\n",
    "    #     if len(component)<3:\n",
    "    #         for node in component:\n",
    "    #             G_sampled.remove_node(node)\n",
    "                \n",
    "    n_nodes = len(G_sampled.nodes())\n",
    "    n_edges = len(G_sampled.edges())\n",
    "    \n",
    "    degree = dict(G_sampled.degree)\n",
    "    avg_degree = round(np.average(list(degree.values())),4)\n",
    "    std_degree = round(np.std(list(degree.values())),4)\n",
    "    \n",
    "    edge_weights = nx.get_edge_attributes(G_sampled, 'weight')\n",
    "    edge_weights_values = list(edge_weights.values())\n",
    "    avg_weight = np.mean(edge_weights_values)\n",
    "    std_weight = np.std(edge_weights_values)\n",
    "    \n",
    "    density = round(nx.density(G_sampled),4)\n",
    "    \n",
    "    n_cc = nx.number_connected_components(G_sampled)\n",
    "    components = list(nx.connected_components(G_sampled))\n",
    "    component_sizes = [len(component) for component in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    \n",
    "    clustering = round(nx.average_clustering(G_sampled),4)\n",
    "    clustering_coeffs = nx.clustering(G_sampled)\n",
    "    clustering_coeffs_values = list(clustering_coeffs.values())\n",
    "    std_clustering = np.std(clustering_coeffs_values)\n",
    "    \n",
    "    \n",
    "    complete_partition_sampled = community.best_partition(G_sampled, resolution=1)\n",
    "    modularity = community.modularity(complete_partition_sampled, G_sampled)\n",
    "    \n",
    "    pkl.dump(complete_partition_sampled, open(Path_Communities, \"wb\"), protocol=4) \n",
    "    \n",
    "    n_comm_10 = len(set(complete_partition_sampled.values()))\n",
    "    \n",
    "    community_sizes_10 = Counter(complete_partition_sampled.values())\n",
    "    \n",
    "    biggest_community_size_10 = max(community_sizes_10.values())\n",
    "    \n",
    "    print(n_nodes, n_edges, n_cc, modularity)\n",
    "    \n",
    "    assortativity = nx.degree_assortativity_coefficient(G_sampled)\n",
    "    \n",
    "    # Calcula o NMI entre os arquivos pickle\n",
    "    nmi, intersection = calculate_nmi('whatsapp/backbone/comm_complete_whatsapp_1.pkl', Path_Communities)\n",
    "    \n",
    "    parameter = parameter\n",
    "    sample = sample\n",
    "    batch = batch\n",
    "    \n",
    "    network = 'WhatsApp'\n",
    "    type_network = \"sample_FF\"\n",
    "    snap = \"October\"\n",
    "    \n",
    "    list_results.append((network, type_network, snap, n_nodes, n_edges, avg_weight, std_weight, avg_degree, std_degree, \n",
    "                         density, clustering, std_clustering, n_cc, largest_component_size, n_comm_10,biggest_community_size_10, modularity, assortativity, parameter, sample, batch,intersection, nmi))\n",
    "    \n",
    "    df = pd.DataFrame(list_results, columns=['Network', 'Type', 'Snapshot', '# Nodes', '# Edges', 'Avg. Weight', 'Std. Weight',\n",
    "                                             'Avg. Degree','Std. Degree','Density', 'Avg. Clustering', 'Std. Clustering','# Components',\n",
    "                                             '# Big. Component Size','# Communities', '# Big. Community Size', 'Modularity', \"Assortivity\", \n",
    "                                             \"Parameter\",\"Net. Sample\", \"Batch\", \"intersection\",\"nmi\"])\n",
    "    del G_sampled\n",
    "    # del G\n",
    "    # del partition\n",
    "    del complete_partition_sampled\n",
    "\n",
    "    df.to_csv(\"whatsapp/ForestFire/communities_backbone/networks_whatsapp_samples_characterization.csv\", mode='a', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42135b21-2ece-42ff-9495-9666241e307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_forest_fire_sample_networks_backbone(dataset):\n",
    "    header = True\n",
    "    k = 1\n",
    "    while k <= 10:\n",
    "        print(k)\n",
    "        for i in range(1,11,1):\n",
    "            for j in range(1,10,1):\n",
    "                sample = \"whatsapp/ForestFire/backbone_samples_complete/\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch'+str(k)+'_'+str(i)+'.csv'\n",
    "                Path_Communities = \"whatsapp/ForestFire/communities_backbone/\"+dataset+\"_FF_batch_\"+str(k)+\"_net_percent_\"+str(i/10)+\"_probability_\"+str(j/10)+\".pkl\"\n",
    "                characterize_sampled_network_backbone_ff(sample, Path_Communities, dataset, \"sample Whatsapp - Forest Fire\", \"batch_\"+str(k)+'_sample_'+str(i/10)+'_probaability_'+str(j/10), 1, str(j/10), header, str(i/10), str(k))\n",
    "                header = False\n",
    "            # print(\"saved_community_\"+dataset+\"_\"+str(i/10)+\"_probability_\"+str(j/10)+'_forest_fire_edge_list_batch_'+str(k)+'_'+str(i)+'_.csv')\n",
    "            header = False\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b471c9-fba6-4025-bc2d-7835a138aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characterize_forest_fire_sample_networks_backbone('whatsapp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9babd7e-aea3-4472-a2fd-64a6be1bf61c",
   "metadata": {},
   "source": [
    "### 6.1 Calculate samples NMI mean and std per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4b358-497e-45b5-a7b2-6eec8ab4bf6f",
   "metadata": {},
   "source": [
    "#### In this code I load csv with samples and samples communities info and I group them per parameter and after that I calculate NMI mean and std\n",
    "#### At the end I store them in a csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf798210-472f-4c3f-8835-e780946db407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forest_fire_original_whatsapp = pd.read_csv('ForestFire/communities_backbone/networks_whatsapp_samples_characterization.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa222a-9bbc-4241-a12f-3a3dfb14c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp = df_forest_fire_original_whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cec1e-7777-4dca-b25f-a2c3b22aeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c50cae-dea0-49b9-9cac-b31c9cc5a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp[\"nmi_mean\"]=0\n",
    "df_FF_NMI_whatsapp[\"std\"]=0\n",
    "df_FF_NMI_whatsapp[\"nmi_std\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3790a5-e347-455b-83c3-1a1bee2794b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp['nmi_norm'] = df_FF_NMI_whatsapp['nmi'] / 0.5875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7a2ed-9bfd-4ff5-a390-90bbfc1dc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp['nmi_mean'] = df_FF_NMI_whatsapp.groupby(['Net. Sample','Parameter'])['nmi_norm'].transform(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2b0a8-cd26-481c-a8c4-bd0615d713eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp['std_1'] = df_FF_NMI_whatsapp.groupby(['Net. Sample','Parameter'])['nmi_norm'].transform(lambda x: x.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7820a3-34cf-4e32-b55f-54001c8de13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FF_NMI_whatsapp[\"nmi_std\"] = df_FF_NMI_whatsapp.apply(lambda x: '{:.2f}±{:.2f}'.format(x[\"nmi_mean\"], x[\"std_1\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b95f02-5968-43c0-9a49-2835c577d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = df_FF_NMI_whatsapp[['Batch','Net. Sample', 'Parameter','nmi_std']].copy()\n",
    "new_df_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1572c-4a21-41d0-a373-106ac19bdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df_2 = new_df_2[new_df_2['Batch'] == 1]\n",
    "rslt_df_2.drop('Batch', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbca32-6175-4c4d-89a0-a1bdea2046e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2 = rslt_df_2.pivot(index=[\"Net. Sample\"], columns=[\"Parameter\"])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pivoted_2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13934e83-6351-4b75-b611-755c9129958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_2[:100].to_csv(\"ForestFire/communities_backbone/networks_nmi_whatsapp_samples_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf915673-ca46-498e-bebb-1b0e63ae2881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
